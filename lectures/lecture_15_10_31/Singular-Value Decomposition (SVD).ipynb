{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Singular-Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Everything I know about the SVD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 15 years ago, almost nobody knew about the SVD, even in this course -- the SVD was not very well known.\n",
    "It seems to have become more popular lately. It was known only by those who had taken numerical analysis course.\n",
    "\n",
    "Can talk about eigenvalues and singular values. Eigenvalues are taught in the first linear algebra course, but singular values not. Why? It has to do with quantum mechanics (just a theory that this is the case). Everything is a symmetric or Hermitian matrix, so only eigenvalues are important. All the big linear algebra books got written in that era, so eigenvalues got stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the unit sphere (surface of the unit ball), $\\{x \\in \\mathbb{R}^n: \\| x \\| = 1 \\}$ and we apply a matrix $A$ to each one. We get (intuition) some sort of ellipsoid. It could collapse (be singular).\n",
    "\n",
    "The obvious question: what is the length of the largest stretching. It's sometimes called $\\| A \\|$.\n",
    "How could I calculate it?  \n",
    "\n",
    "The wrong answer to the direction is the eigenvector, and the length is the eigenvalue of $A$.\n",
    "\n",
    "$$ \\| A \\| = \\sup_{x \\ne 0} \\frac {\\| Ax \\| } { \\| x \\| }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But matrices can have negative eigenvalues, or even complex. So this doesn't seem right -- meant the absolute value of the \"largest\"  eigenvalue of A.  It's  correct for a positive-definite symmtric matrix, but *not* for general matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is that it is the largest **singular value** of $A$; the direction is the corresponding **singular vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is **way more important** than eigenvalues -- even if math courses have not yet caught up with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is a factorization:\n",
    "\n",
    "$$A = U \\, \\Sigma \\, V^\\mathrm{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$U$ and $V$ are orthogonal matrices, $U^T \\, U = I$ and $V^T \\, V = I$.\n",
    "Recall that orthogonal matrices are just rotations and reflections, that leave the sphere the same.\n",
    "The columns are orthonormal. (Note that the matrix tends not to be called \"orthonormal\", even though it should be. cf. Strang)\n",
    "\n",
    "$\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)$\n",
    "\n",
    "with $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge 0$.\n",
    "\n",
    "This means that an arbitrary matrix can be decomposed as a rotation, followed by a stretch, followed by another rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematicians like to ask questions about **invariants**.\n",
    "\n",
    "**Invariant definition of singular values**: The data that is preserved if you multiply $A$ on the left and right by orthogonal matrices.\n",
    "\n",
    "i.e. the collection of matrices that you get by hitting a given matrix on the left and right by orthogonal matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons that the SVD is so useful is its \"best approximation\" property.\n",
    "\n",
    "First let's write down the SVD in vector, rather than matrix, notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\sum_{i=1}^{\\mathrm{rank}(A)} \\sigma_i u_i v_i^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $A_k := \\sum_{i=1}^k \\sigma_i u_i v_i^T$  (a partial sum).\n",
    "\n",
    "**Theorem**: $A_k$ is the best rank-$k$ approximation to $A$, as measured in the \"2-norm\" or the \"Frobenius norm\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-norm:  \n",
    "\n",
    "$$\\| A \\|_2 = \\sigma_{\\mathrm{max}}(A)$$\n",
    "\n",
    "Frobenius norm:\n",
    "\n",
    "$$\\| A \\|_F = \\sqrt{\\sum X_{ij}^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** (homework): Grab an image and compress it by calculating $A_k$. It usually has 3 components (red, green and blue). Do the SVD separately on each component. \"Take a selfie and SVD yourself!\"\n",
    "\n",
    "Rank-$1$ matrices look like stripes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application**: This theorem (\"useful little math fact\") turns into a very useful computational tool: compressing matrices, keeping the most essential information (hopefully...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The SVD is so easy to try on any data set, that you'd be silly not to.\" (It may not be the last word -- maybe not specialised enough for any given application.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The things people do with the SVD are all sort of similar.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose have matrix $A$ such that each column points exactly in the same direction, i.e. is of rank $1$. So $A_1 = A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_1 = \\sigma_1 u_1 v_1^T$  is a rank-$1$ matrix\n",
    "\n",
    "$A_k$ is a rank-$k$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose have a matrix such that every column is *approximately* a multiple of each other column. A mathematician would say it would have full rank, but the point is that it is approximately of rank $1$, so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A \\approx \\sigma_1 u_1 v_1^T,$$\n",
    "\n",
    "i.e. each column is approximately equal to $u_1$.\n",
    "\n",
    "But it may be that the columns approximately live on some $k$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Alan worked at Pixar. The artists were complaining that they wanted a particular shade of green. They were still using film, and the process of going from the color on the computer screen to the final projected color.\n",
    "\n",
    "Somebody measured the actual wavelength content and gave 101 components for each measurement. There were only 3 singular values that were not close to 0. These correspond, in some sense, to red, green and blue.\n",
    "\n",
    "If the data lives on a hyperplane, the SVD will find it and reveal that structure (a \"low-dimensional flatness-detecting tool\"). Unfortunately things in the real world are curved. There are things that you can do to make the data flat if it is not already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **key fact** is that $u_1$, $u_2$,... represent the best basis for the column space.\n",
    "\n",
    "Suppose somebody gives you a vector and you must reduce the data to a single number. Would give the mean. (All the time we try to compress data down.)\n",
    "\n",
    "If you give me a whole lot (1000) vectors, and you must give back a guess of the best vector that describes them all, can give the mean of each component, or the median of each component, of $u_1$.\n",
    "\n",
    "$u_1$ is the \"best direction\" that represents every column of a matrix.  E.g. in the color example, the average color is grey.\n",
    "\n",
    "$u_2$ is the \"best correction\" to $u_1$.\n",
    "\n",
    "Compare Media Lab: eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $U = [u_1 u_2 u_3]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} = U^t \\cdot \\begin{pmatrix} \\cdot \\\\ \\cdot \\\\ \\vdots \\\\ \\cdot \\end{pmatrix}$$\n",
    "\n",
    "gives the components of the projection onto the 3-dimensional hyperplane spanned by $u_1$, $u_2$, $u_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information retrieval application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix: Every column is a document; each row corresponds to a word.\n",
    "There are a few things you might do to populate the columns of the matrix,\n",
    "\n",
    "e.g. $A_{ij} = $ number of times that word $i$ appearsin document $j$\n",
    "\n",
    "or an incidence matrix: $1$ if word $i$ appears.\n",
    "\n",
    "$u_1$ is an \"average, representative document\" \n",
    "\n",
    "If have a non-negative matrix, then $u_1$ and $v_1$ will be non-negative, but $u_2$ etc. may have negative numbers, which is problematic\n",
    "\n",
    "Can compress documents by the $u$s, or go the other way and compress words with the $v$.\n",
    "E.g. classify new documents as they come in.\n",
    "\n",
    "It seems that it doesn't work that well, but it's not crazy either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovered by mathematicians in the 18th / 19th century in the context of integral operators.\n",
    "\n",
    "Gene Gollub (Stanford): Informed people that the SVD is really valuable and came up with some algorithms for calculating it. (His license plates were \"DR SVD\" and \"PROF SVD\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If you're coming from another area where the SVD is not known, you can bring the SVD to your field and become a hero and win prizes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few people know about the GSVD (Generalized SVD). The problem with it is that every definition of it looks too complicated to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the SVD you get radii but not angles.\"\n",
    "\n",
    "Right triangle with matrices $A$ and $B$ on the sides, and $(A^T A + B^T B)^{1/2}$ -- matrix version of trigonometry (\"there is a matrix version of everything, and it's always more fun!\")\n",
    "\n",
    "Can you talk about the angle? You want to write\n",
    "\n",
    "$$\\sin \\theta = \\frac{B}{(A^T A + B^T B)^{1/2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It almost does have a meaning. There's a collection of angles $\\theta_1$, $\\theta_2$ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea in 2D: If have a line, want to know if the line is close to the $x$-axis, close to $y$-axis, in the middle, etc.\n",
    "\n",
    "Graph $(x,y)$ where $y$ is linearly related to $x$.\n",
    "\n",
    "One thing can do in higher dimensions: \n",
    "\n",
    "Let $A_1$ be an $m_1 \\times n$ matrix.\n",
    "Let $A_2$ be an $m_2 \\times n$ matrix.  \n",
    "\n",
    "I.e. they have the same number of columns, but don't care about the rows.\n",
    "\n",
    "\n",
    "In 2D, the set $\\left \\{\\begin{pmatrix} a_1 t \\\\ a_2 t \\end{pmatrix}: t \\in \\mathbb{R} \\right \\}$ is a line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column of the identity is a \"one-hot vector\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a picture in $m = m_1 + m_2$ dimensional space.\n",
    "Look at the hyperplane: span of $\\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix}$, i.e.\n",
    "\n",
    "$\\left \\{\\begin{pmatrix} A_1 t \\\\ A_2 t \\end{pmatrix}: t \\in \\mathbb{R^n} \\right \\}$\n",
    "\n",
    "The GSVD will tell you to what extent some of the directions are close to the two \"coordinate axes\",\n",
    "i.e. in how many directions $\\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix}$ is \"shallow\" vs \"steep\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the GSVD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GSVD is a factorization: \n",
    "\n",
    "$$A_1 = U C \\cdot X$$\n",
    "\n",
    "$$A_2 = U S \\cdot X$$\n",
    "\n",
    "with $C = \\mathrm{diag}(c_1, \\ldots, c_n)$\n",
    "\n",
    "$S  = \\mathrm{diag}(s_1, \\ldots, s_n)$\n",
    "\n",
    "with $c_i^2 + s_i^2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD the bad way:\n",
    "\n",
    "(Not how you should calculate it on a computer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "$$A A^T = U \\Sigma^2 U^T$$\n",
    "\n",
    "Can now calculate eigenvalues and eigenvectors. Can calculate $A A^T$ fast, but you'll kill the small singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple way to calculate the GSVD:\n",
    "\n",
    "eig(A * inv(A^T A + B^T B)^{1/2}))\n",
    "\n",
    "will give $\\cos \\theta_1$, $\\cos \\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People don't understand the GSVD well. But there are bound to be lots of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\det(A^T A - \\lambda B^T B) = 0$ then $\\lambda$ is a generalized eigenvalue of $(A^T A, B^T B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative name: pseudo-SVD\n",
    "\n",
    "Under the right conditions ($B$ is full column rank), it's the singular values of $A B^+$, where $B^+$ is the pseudo-inverse of $B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data $x \\in \\mathbb{R}^n$, will apply a function $f$ that will compress it, e.g. turn a handwritten digit into $(p_0, \\ldots, p_9)$ that represent the probabilities of it being a $0$, $1$ etc.\n",
    "\n",
    "Machine learning is about finding the $f$ that gives the best prediction, usually within some space of functions that you allow yourself.\n",
    "\n",
    "\"Learning\" means that we have found an $f$ such that $\\mathrm{argmax}_i (p_i)$ is likely to represent what $x$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is a \"poor man's way of coming up with one of these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that people don't usually want *all* the singular values; rather, they usually want just the first few singular values and the corresponding singular vectors. There are very different algorithms for these two cases (all vs. a few). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
